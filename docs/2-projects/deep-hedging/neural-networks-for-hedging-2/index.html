<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Neural Networks for Hedging: Part 2"><meta property="og:title" content="Neural Networks for Hedging: Part 2" />
<meta property="og:description" content="Neural Networks for Hedging: Introduction This is Part 2, Part 1 can be found here. All the code for both parts is available on Github. Our goal is to implement the 2018 (published in 2019) paper by Beuhler et al., &ldquo;Deep Hedging&rdquo;, using PyTorch.
In Part 1, we built a very simple neural net with two hidden layers. As a first test-case, we used it to approximate the Black-Scholes delta function, when the true analytical delta is used as the loss function." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://www.ralphrudd.com/docs/2-projects/deep-hedging/neural-networks-for-hedging-2/" />
<meta property="article:published_time" content="2019-11-27T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-11-27T00:00:00+00:00" />
<title>Neural Networks for Hedging: Part 2 | The Commonplace Book</title>
<link rel="icon" href="/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/book.min.e3ca513519c1bf3d15c9b2dbc36617f10bb187b6dbb9c5ceb8e24cae1a1e8017.css" integrity="sha256-48pRNRnBvz0VybLbw2YX8Quxh7bbucXOuOJMrhoegBc=">


<script defer src="/en.search.min.2b62a6bfb0f048ac96d45936f6dacaa0327a467a8a8b4a2e429af6c35c4d5c55.js" integrity="sha256-K2Kmv7DwSKyW1Fk29trKoDJ6RnqKi0ouQpr2w1xNXFU="></script>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="flex container">

    <aside class="book-menu fixed">
      <nav>
<h2 class="book-brand">
  <a href="http://www.ralphrudd.com/"><span>The Commonplace Book</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>





    

  
  





 
  
    




  
  <ul>
    
      
        

  <li  class="book-section-flat" >
    
      <span>Projects</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Deep Hedging</span>
    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/2-projects/deep-calibration/neural_networks_for_calibration_1/" >
      Neural Networks for Calibration
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Deep Hedging</span>
    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/2-projects/deep-hedging/neural-networks-for-hedging-2/"  class="active">
      Neural Networks for Hedging: Part 2
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/2-projects/deep-hedging/neural-networks-for-hedging-1/" >
      Neural Networks for Hedging: Part 1
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li  class="book-section-flat" >
    
      <span>Areas</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/docs/3-areas/brazilian-jiu-jitsu/" >
      Brazilian Jiu Jitsu
  </a>


    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/3-areas/brazilian-jiu-jitsu/guard/" >
      Playing Guard
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/3-areas/brazilian-jiu-jitsu/guard_passing/" >
      Guard Passing
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Programming</span>
    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/3-areas/programming/working_with_pelican/" >
      Working with Pelican: Assorted Notes
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/3-areas/programming/minimal_pelican_webpage/" >
      A Minimal Webpage with Pelican
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/3-areas/programming/becoming_a_quantitative_developer/" >
      Becoming a Quantitative Developer (Deprecated)
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li  class="book-section-flat" >
    
      <span>Resources</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li  class="book-section-flat" >
    
      

  <a href="/docs/5-archives/" >
      Archives
  </a>


    

    






  </li>


      
    
  </ul>
  



  











</nav>


<script>
(function() {
  var menu = document.querySelector("aside.book-menu nav");
  addEventListener("beforeunload", function(event) {
    localStorage.setItem("menu.scrollTop", menu.scrollTop);
  });
  menu.scrollTop = localStorage.getItem("menu.scrollTop");
})();
</script>

    </aside>

    <div class="book-page">
      <header class="flex align-center justify-between book-header">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>
  <strong>Neural Networks for Hedging: Part 2</strong>
</header>

      
<article class="markdown"><h1 id="neural-networks-for-hedging">Neural Networks for Hedging:</h1>
<h2 id="introduction">Introduction</h2>
<p>This is Part 2, Part 1 can be found <a href="https://www.ralphrudd.com/docs/2-projects/deep-hedging/neural-networks-for-hedging-1/">here</a>. All the code for both parts is available on <a href="https://github.com/IkeThermite/Buehler-et-al-Deep-Hedging">Github</a>.
Our goal is to implement the 2018 (published in 2019) paper by Beuhler et al., <a href="https://www.tandfonline.com/doi/full/10.1080/14697688.2019.1571683">&ldquo;Deep Hedging&rdquo;</a>, using <a href="https://pytorch.org">PyTorch</a>.</p>
<p>In <a href="http://www.ralphrudd.com/blog/2019/10/22/neural-networks-for-hedging-1">Part 1</a>, we built a very simple neural net with two hidden layers. As a first test-case, we used it to approximate the Black-Scholes delta function, when the true analytical delta is used as the loss function.
Then, we attempted to learn the appropriate delta when the loss function was the profit and loss that arises from selling a vanilla call option and hedging with the underlying asset. Both of these experiments were done in a <em>single timestep model</em>, such that we sell the option today, implement our hedge, and the option expires one timestep from now, where we realize the profit or loss on our implemented hedge.</p>
<p>Here, we attempt to learn the two timestep hedge, but without making the network any deeper. Instead, we build a <em>recurrent neural net</em>.</p>
<h2 id="a-recurrent-neural-network">A Recurrent Neural Network</h2>
<p>We will use the same neural network structure as in <a href="http://www.ralphrudd.com/blog/2019/10/22/neural-networks-for-hedging-1">Part 1</a>, but with the addition of a <em>hidden state</em>.</p>
<p>This neural network, with two hidden layers as before, is traversed at each timestep and receives as input the price of the underlying asset for that time, as well as the delta from the previous timestep, representing the hidden state. At each timestep, the network outputs the delta for that time.</p>
<p><img src="/img/simple_recursive_network_nn_svg.png#center" alt="A simple neural network."></p>
<p>The network is <em>recurrent</em>, as the output at the current time (the hidden state), feeds into the network as an input at the next timestep. An alternative in our case would be to have a separate neural network for each timestep, with two hidden layers each. This would work, but would greatly increase the number of parameters that need to be trained.</p>
<p>In PyTorch, the network is defined as follows:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Net</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, num_neurons):
        super(Net, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>lin1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">2</span>, num_neurons)
        self<span style="color:#f92672">.</span>sig1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sigmoid()
        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(num_neurons)
        self<span style="color:#f92672">.</span>lin2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(num_neurons, <span style="color:#ae81ff">1</span>)
        self<span style="color:#f92672">.</span>sig2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sigmoid()
        

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, X):
        batch_size, steps, _ <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape
        output <span style="color:#f92672">=</span> []
        self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>rand(batch_size, <span style="color:#ae81ff">1</span>)
        
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(steps):
            self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((X[:,i,:], self<span style="color:#f92672">.</span>y), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
            self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>sig1(self<span style="color:#f92672">.</span>lin1(self<span style="color:#f92672">.</span>y)))
            self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sig2(self<span style="color:#f92672">.</span>lin2(self<span style="color:#f92672">.</span>y));
            output<span style="color:#f92672">.</span>append(self<span style="color:#f92672">.</span>y)
        
        <span style="color:#66d9ef">return</span> output, self<span style="color:#f92672">.</span>y       
</code></pre></div><p>The recurrent nature of the net comes from the <code>for</code> loop inside the <code>forward</code> method. There are two further modifications to the structure from Part 1: an additional sigmoid function on the first linear layer and a batch normalization procedure applied before the second layer. Batch normalization was recommended in the original paper and greatly increased the learning rate of the network.</p>
<h2 id="learning-the-two-step-hedge">Learning The Two-step Hedge</h2>
<p>We attempt to learn the best hedge position at each timestep by minimizing the terminal profit and loss.</p>
<p>At $t_0$ we sell a call option for $C_0$ and buy $\delta_0$ units of the underlying stock, $S_0$.
At $t_1$ we rebalance our hedge by adjusting our holding in the stock (our final position must be $\delta_1$). Finally, at $T = t_2$ we have to pay out the payoff of the option (if positive) and close out our position. Thus, the function we want to minimize looks like</p>
<p>$$
\delta_1(S_2 - S_1) + \delta_0(S_1 - S_0) + C_0 - (S_2 - K)^+
$$</p>
<p>and our network must provide both $\delta_0$ and $\delta_1$. Again these should be close to the Black-Scholes deltas, with the difference accounting for the discrete-time nature of the hedge.</p>
<p>To create the training set, we need to price the call options at $t_0$ as well as simulate realizations for the underlying asset at $t_1$ and $t_2$.</p>
<p>The training of the RNN is illustrated below and contrasted to the analytical Black-Scholes delta at each timestep.</p>
<p><img src="/img/rnn_bs_hedge.gif#center" alt="A simple neural network."></p>
<h2 id="next-steps">Next Steps</h2>
<p>Roughly:</p>
<ol>
<li>What is happening in the tails at $t_1$? Are there simply too few samples there to learn the appropriate shape?</li>
<li>Expand the two-period hedge to a $n$-period hedge. I am dissatisfied that the PyTorch <code>RNNCell</code> couldn't match my requirements here and that's worth investigating. I'll also need to get significantly more skilled at manipulating arrays.</li>
<li>As before, I still intend to change the underlying model to Heston. For this, we'll need two-dimensional inputs, as we'll require a derivative trading instrument to hedge the volatility.</li>
</ol>
</article>

      <div class="book-footer justify-between">
  

  

  
</div>

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  displayMath: [['$$','$$']],
  processEscapes: true,
  processEnvironments: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
  TeX: { equationNumbers: { autoNumber: "AMS" },
       extensions: ["AMSmath.js", "AMSsymbols.js"] }
}
});
MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});

MathJax.Hub.Config({

TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script> 

      
    </div>

    
  

  <aside class="book-toc levels-3 fixed">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#a-recurrent-neural-network">A Recurrent Neural Network</a></li>
    <li><a href="#learning-the-two-step-hedge">Learning The Two-step Hedge</a></li>
    <li><a href="#next-steps">Next Steps</a></li>
  </ul>
</nav>
  </aside>



  </main>

  
</body>

</html>
