<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Neural Networks for Calibration: Part 1"><meta property="og:title" content="Neural Networks for Calibration: Part 1" />
<meta property="og:description" content="Neural Networks for Calibration: Part 1 Introduction The purpose of this page is track my experiments with using neural networks to move the hard work of model calibration &ldquo;offline&rdquo;.
My primary resource is the 2016 paper by Hernandez, &ldquo;Model Calibration with Neural Networks&rdquo;. This served as the inspiration for a &ldquo;2017 Financial Mathematics Team Challenge Project&rdquo;, which was supervised by Josef Teichman. Finally, in 2018 Nicolau Haussamer wrote &ldquo;Model Calibration with Machine Learning&rdquo; as his short dissertation, in partial fulfillment of his MPhil in Mathematical Finance at the University of Cape Town." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://www.ralphrudd.com/docs/2-projects/deep-calibration/neural_networks_for_calibration_1/" />
<meta property="article:published_time" content="2020-01-03T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-01-03T00:00:00+00:00" />
<title>Neural Networks for Calibration: Part 1 | The Commonplace Book</title>
<link rel="icon" href="/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/book.min.e3ca513519c1bf3d15c9b2dbc36617f10bb187b6dbb9c5ceb8e24cae1a1e8017.css" integrity="sha256-48pRNRnBvz0VybLbw2YX8Quxh7bbucXOuOJMrhoegBc=">


<script defer src="/en.search.min.5a8848b7de43e13b12fa5f64098bcc9ad6223eb0839b856138954192da84d441.js" integrity="sha256-WohIt95D4TsS&#43;l9kCYvMmtYiPrCDm4VhOJVBktqE1EE="></script>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="flex container">

    <aside class="book-menu fixed">
      <nav>
<h2 class="book-brand">
  <a href="http://www.ralphrudd.com/"><span>The Commonplace Book</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>





    

  
  





 
  
    




  
  <ul>
    
      
        

  <li  class="book-section-flat" >
    
      <span>Projects</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Deep Hedging</span>
    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/2-projects/deep-calibration/neural_networks_for_calibration_1/"  class="active">
      Neural Networks for Calibration: Part 1
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Deep Hedging</span>
    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/2-projects/deep-hedging/neural-networks-for-hedging-2/" >
      Neural Networks for Hedging: Part 2
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/2-projects/deep-hedging/neural-networks-for-hedging-1/" >
      Neural Networks for Hedging: Part 1
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li  class="book-section-flat" >
    
      <span>Areas</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/docs/3-areas/brazilian-jiu-jitsu/" >
      Brazilian Jiu Jitsu
  </a>


    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/3-areas/brazilian-jiu-jitsu/guard/" >
      Playing Guard
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/3-areas/brazilian-jiu-jitsu/guard_passing/" >
      Guard Passing
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Programming</span>
    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/3-areas/programming/working_with_pelican/" >
      Working with Pelican: Assorted Notes
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/3-areas/programming/minimal_pelican_webpage/" >
      A Minimal Webpage with Pelican
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/3-areas/programming/becoming_a_quantitative_developer/" >
      Becoming a Quantitative Developer (Deprecated)
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li  class="book-section-flat" >
    
      <span>Resources</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li  class="book-section-flat" >
    
      

  <a href="/docs/5-archives/" >
      Archives
  </a>


    

    






  </li>


      
    
  </ul>
  



  











</nav>


<script>
(function() {
  var menu = document.querySelector("aside.book-menu nav");
  addEventListener("beforeunload", function(event) {
    localStorage.setItem("menu.scrollTop", menu.scrollTop);
  });
  menu.scrollTop = localStorage.getItem("menu.scrollTop");
})();
</script>

    </aside>

    <div class="book-page">
      <header class="flex align-center justify-between book-header">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>
  <strong>Neural Networks for Calibration: Part 1</strong>
</header>

      
<article class="markdown"><h1 id="neural-networks-for-calibration-part-1">Neural Networks for Calibration: Part 1</h1>
<h2 id="introduction">Introduction</h2>
<p>The purpose of this page is track my experiments with using neural networks to move the hard work of model calibration &ldquo;offline&rdquo;.</p>
<p>My primary resource is the 2016 paper by Hernandez, <a href="">&ldquo;Model Calibration with Neural Networks&rdquo;</a>.
This served as the inspiration for a <a href="">&ldquo;2017 Financial Mathematics Team Challenge Project&rdquo;</a>, which was supervised by Josef Teichman. 
Finally, in 2018 Nicolau Haussamer wrote <a href="">&ldquo;Model Calibration with Machine Learning&rdquo;</a> as his short dissertation, in partial fulfillment of his MPhil in Mathematical Finance at the University of Cape Town.</p>
<h2 id="a-first-network">A First Network</h2>
<p>In statistics, you begin with the normal distribution.
In quantitative finance, you begin with the Black-Scholes model.</p>
<p>My first attempt is to learn the inverse of the Black-Scholes put option price as a function of a single parameter; the implied volatility $\sigma$.</p>
<p>I construct a simple neural network with two hidden layers of 16 nodes each, both with the SoftMax activation function. In the middle, I perform a <a href="https://arxiv.org/abs/1502.03167">batch normalization</a>, but I do not attempt to train the weights of the normalization layer.</p>
<p>In <a href="https://pytorch.org/">PyTorch</a>, the network is defined as:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Net</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, num_neurons):
        super(Net, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>lin1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1</span>, num_neurons)
        self<span style="color:#f92672">.</span>softplus1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Softplus()
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(num_neurons, affine<span style="color:#f92672">=</span>False)
        self<span style="color:#f92672">.</span>lin2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(num_neurons, <span style="color:#ae81ff">1</span>)
        self<span style="color:#f92672">.</span>softplus2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Softplus()
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, out):
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>softplus1(self<span style="color:#f92672">.</span>lin1(out))
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn2(out)
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>softplus2(self<span style="color:#f92672">.</span>lin2(out))
        <span style="color:#66d9ef">return</span> out
</code></pre></div><p>It can be easily visualized using <a href="">Netron</a>:
<img src="/img/bs_calibration_netron.png#center" alt="The network for calibration the Black-Scholes model."></p>
<h2 id="learning-the-inverse-of-the-black-scholes-pricing-function">Learning the Inverse of the Black-Scholes Pricing Function</h2>
<p>For my training data, I use simulated at-the-money put options with 5 years to expiry. I generate 100 000 pairs of <code>(price, implied_volatility)</code> for $\sigma\in[0.05, 1.5]$.
Using standard stochastic gradient descent, this is what you get:</p>
<p><img src="/img/bs_calibration.gif#center" alt="Animation of learning the inverse Black-Scholes put option pricing function."></p>
<p>Not bad, considering that I haven't done any hyperparameter optimization, but this is the simplest example. 
Let's take one step up.</p>
<h2 id="calibrating-a-two-parameter-model">Calibrating a Two-parameter Model</h2>
<p>Consider the CEV model, a generalization of Black-Scholes:
$$ dS_t = rS_t dt + \sigma S^\alpha_t dW_t $$
Now, given an option price, I want my network to be able to determine the parameter set, $\Theta=(\sigma, \alpha)$ that produced it.</p>
</article>

      <div class="book-footer justify-between">
  

  

  
</div>

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  displayMath: [['$$','$$']],
  processEscapes: true,
  processEnvironments: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
  TeX: { equationNumbers: { autoNumber: "AMS" },
       extensions: ["AMSmath.js", "AMSsymbols.js"] }
}
});
MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});

MathJax.Hub.Config({

TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script> 

      
    </div>

    
  

  <aside class="book-toc levels-3 fixed">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#a-first-network">A First Network</a></li>
    <li><a href="#learning-the-inverse-of-the-black-scholes-pricing-function">Learning the Inverse of the Black-Scholes Pricing Function</a></li>
    <li><a href="#calibrating-a-two-parameter-model">Calibrating a Two-parameter Model</a></li>
  </ul>
</nav>
  </aside>



  </main>

  
</body>

</html>
