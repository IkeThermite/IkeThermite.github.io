<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Neural Networks for Hedging: Part 1"><meta property="og:title" content="Neural Networks for Hedging: Part 1" />
<meta property="og:description" content="Neural Networks for Hedging: Part 1 Introduction The purpose of this page is track my progress implementing the 2018 (published in 2019) paper by Beuhler et al., &ldquo;Deep Hedging&rdquo;. In that paper they use a semi-recurrent deep neural network to calculate the appropriate hedging positions when hedging vanilla options.
They begin in a simulation setting using the Heston model and their network is implemented using TensorFlow. Owing to the current state of machine learning frameworks, as well as to the fact that one of my colleagues already has TensorFlow experience, I have decided to use PyTorch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://www.ralphrudd.com/docs/2-projects/deep-hedging/neural-networks-for-hedging-1/" />
<meta property="article:published_time" content="2019-10-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-10-22T00:00:00+00:00" />
<title>Neural Networks for Hedging: Part 1 | The Commonplace Book</title>
<link rel="icon" href="/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/book.min.e3ca513519c1bf3d15c9b2dbc36617f10bb187b6dbb9c5ceb8e24cae1a1e8017.css" integrity="sha256-48pRNRnBvz0VybLbw2YX8Quxh7bbucXOuOJMrhoegBc=">


<script defer src="/en.search.min.b99a5a54d1f4a4c197c8f773a45c9a3dc19fad315e8042bd77cf997f35b0cc9d.js" integrity="sha256-uZpaVNH0pMGXyPdzpFyaPcGfrTFegEK9d8&#43;ZfzWwzJ0="></script>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="flex container">

    <aside class="book-menu fixed">
      <nav>
<h2 class="book-brand">
  <a href="http://www.ralphrudd.com/"><span>The Commonplace Book</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>





    

  
  





 
  
    




  
  <ul>
    
      
        

  <li  class="book-section-flat" >
    
      <span>Projects</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      <span>Deep Hedging</span>
    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/2-projects/deep-calibration/neural_networks_for_calibration_1/" >
      Neural Networks for Calibration: Part 1
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Deep Hedging</span>
    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/2-projects/deep-hedging/neural-networks-for-hedging-2/" >
      Neural Networks for Hedging: Part 2
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/2-projects/deep-hedging/neural-networks-for-hedging-1/"  class="active">
      Neural Networks for Hedging: Part 1
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li  class="book-section-flat" >
    
      <span>Areas</span>
    

    




  
  <ul>
    
      
        

  <li >
    
      

  <a href="/docs/3-areas/brazilian-jiu-jitsu/" >
      Brazilian Jiu Jitsu
  </a>


    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/3-areas/brazilian-jiu-jitsu/guard/" >
      Playing Guard
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/3-areas/brazilian-jiu-jitsu/guard_passing/" >
      Guard Passing
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
      
        

  <li >
    
      <span>Programming</span>
    

    




  
  <ul>
    
      
        <li>

  <a href="/docs/3-areas/programming/working_with_pelican/" >
      Working with Pelican: Assorted Notes
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/3-areas/programming/minimal_pelican_webpage/" >
      A Minimal Webpage with Pelican
  </a>

</li>
      
    
      
        <li>

  <a href="/docs/3-areas/programming/becoming_a_quantitative_developer/" >
      Becoming a Quantitative Developer (Deprecated)
  </a>

</li>
      
    
  </ul>
  



  </li>


      
    
  </ul>
  



  </li>


      
    
      
        

  <li  class="book-section-flat" >
    
      <span>Resources</span>
    

    




  
  <ul>
    
  </ul>
  



  </li>


      
    
      
        

  <li  class="book-section-flat" >
    
      

  <a href="/docs/5-archives/" >
      Archives
  </a>


    

    






  </li>


      
    
  </ul>
  



  











</nav>


<script>
(function() {
  var menu = document.querySelector("aside.book-menu nav");
  addEventListener("beforeunload", function(event) {
    localStorage.setItem("menu.scrollTop", menu.scrollTop);
  });
  menu.scrollTop = localStorage.getItem("menu.scrollTop");
})();
</script>

    </aside>

    <div class="book-page">
      <header class="flex align-center justify-between book-header">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>
  <strong>Neural Networks for Hedging: Part 1</strong>
</header>

      
<article class="markdown"><h1 id="neural-networks-for-hedging-part-1">Neural Networks for Hedging: Part 1</h1>
<h2 id="introduction">Introduction</h2>
<p>The purpose of this page is track my progress implementing the 2018 (published in 2019) paper by Beuhler et al., <a href="https://www.tandfonline.com/doi/full/10.1080/14697688.2019.1571683">&ldquo;Deep Hedging&rdquo;</a>.
In that paper they use a semi-recurrent deep neural network to calculate the appropriate hedging positions when hedging vanilla options.</p>
<p>They begin in a simulation setting using the Heston model and their network is implemented using <a href="http://tensorflow.org">TensorFlow</a>. 
Owing to the current <a href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/">state of machine learning frameworks</a>, as well as to the fact that one of my colleagues already has TensorFlow experience, I have decided to use <a href="https://pytorch.org">PyTorch</a>.</p>
<h2 id="a-first-network">A First Network</h2>
<p>Beuhler et al. use a neural network with two hidden layers to compute the necessary position (delta) in the underlying for that trading day.
The network is semi-recurrent because the delta for day $i$ is used as an input for the new network at day $i+1$.
It is also <em>very deep</em> as you essentially have two hidden layers per trading day.</p>
<p>To start, I construct the simplest neural network based on their hyperparameters. 
They use $d+15$ nodes per hidden layer, where $d$ is the number of inputs (the underlying assets to be traded).
Using PyTorch, the network is defined as follows:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Imports and Seeds</span>
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> scipy.stats <span style="color:#f92672">as</span> stats
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#f92672">import</span> torch
<span style="color:#f92672">import</span> torch.nn <span style="color:#f92672">as</span> nn
<span style="color:#f92672">import</span> torch.optim <span style="color:#f92672">as</span> optim

np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">0</span>)
torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">0</span>)

<span style="color:#75715e"># Construct Neural Net</span>
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Net</span>(nn<span style="color:#f92672">.</span>Module):
    
    <span style="color:#66d9ef">def</span> __init__(self):
        super(Net, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>lin1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">16</span>)
        self<span style="color:#f92672">.</span>lin2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">1</span>)
        self<span style="color:#f92672">.</span>sigmoid1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sigmoid();
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, S0):
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lin1(S0)
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lin2(out)
        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sigmoid1(out)
        <span style="color:#66d9ef">return</span> out    

net <span style="color:#f92672">=</span> Net()         
</code></pre></div><p>Note that I'm using a sigmoid activation function on the second hidden layer, whereas Beuler et al. use ReLU. 
I'll get back to this.
This simple network can be represented graphically in its entirety:
<img src="/img/simple_network_nn_svg.png#center" alt="A simple neural network."></p>
<p>The input (single node on the left) will be the normalized price of the underlying asset, whereas the output (single node on the right) would be optimal trading position in that asset.</p>
<p>The mathematical operations being performed by each layer can be easily 
visualized by saving the Pytorch model and then importing it into Netron.
(Note: In theory, TensorBoard is really the way to do this, but I was having compatibility issues.)</p>
<p><img src="/img/simple_network_netron.png#center" alt="A simple network represented as an execution path."></p>
<h2 id="learning-the-black-scholes-delta">Learning The Black-Scholes Delta</h2>
<p>To check that the network is implemented correctly, I test whether it can approximate a <em>known</em> non-linear function. 
For this, I generate a random sample of initial stock price values and use as a target the corresponding Black-Scholes deltas for a call option struck at $100$, one week from maturity.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Functions</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">d1</span>(S0, K, T, r, sigma):
    <span style="color:#66d9ef">return</span> (np<span style="color:#f92672">.</span>log(S0 <span style="color:#f92672">/</span> K) <span style="color:#f92672">+</span> (r <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> sigma <span style="color:#f92672">*</span><span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> T) <span style="color:#f92672">/</span> (sigma <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(T))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">d2</span>(S0, K, T, r, sigma):
    <span style="color:#66d9ef">return</span> d1(S0, K, T, r, sigma) <span style="color:#f92672">-</span> sigma <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(T)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">price_put_BS</span>(S0, K, T, r, sigma):
    <span style="color:#66d9ef">return</span> (stats<span style="color:#f92672">.</span>norm<span style="color:#f92672">.</span>cdf(<span style="color:#f92672">-</span>d2(S0, K, T, r, sigma)) <span style="color:#f92672">*</span> K <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>r <span style="color:#f92672">*</span> T) <span style="color:#f92672">-</span> 
                           stats<span style="color:#f92672">.</span>norm<span style="color:#f92672">.</span>cdf(<span style="color:#f92672">-</span>d1(S0, K, T, r, sigma)) <span style="color:#f92672">*</span> S0)
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">price_call_BS</span>(S0, K, T, r, sigma):
    <span style="color:#66d9ef">return</span> (stats<span style="color:#f92672">.</span>norm<span style="color:#f92672">.</span>cdf(d1(S0, K, T, r, sigma)) <span style="color:#f92672">*</span> S0 <span style="color:#f92672">-</span> 
            stats<span style="color:#f92672">.</span>norm<span style="color:#f92672">.</span>cdf(d2(S0, K, T, r, sigma)) <span style="color:#f92672">*</span> K <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>r <span style="color:#f92672">*</span> T))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">delta_put_BS</span>(S0, K, T, r, sigma):
    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>stats<span style="color:#f92672">.</span>norm<span style="color:#f92672">.</span>cdf(<span style="color:#f92672">-</span>d1(S0, K, T, r, sigma))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">delta_call_BS</span>(S0, K, T, r, sigma):
    <span style="color:#66d9ef">return</span> stats<span style="color:#f92672">.</span>norm<span style="color:#f92672">.</span>cdf(d1(S0, K, T, r, sigma));
<span style="color:#75715e"># Parameters</span>
filename <span style="color:#f92672">=</span> <span style="color:#e6db74"></span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">bs_delta_1</span><span style="color:#e6db74">&#39;</span>
S0 <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
K <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
T <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">50</span>
r <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.05</span>
sigma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>
num_samples <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>;
num_epochs  <span style="color:#f92672">=</span> <span style="color:#ae81ff">15</span>;
batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>;
S0_lower_bound <span style="color:#f92672">=</span> <span style="color:#ae81ff">90</span>;
S0_upper_bound <span style="color:#f92672">=</span> <span style="color:#ae81ff">110</span>;

uniform_samples <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(num_samples, <span style="color:#ae81ff">1</span>)
S0_values <span style="color:#f92672">=</span> (S0_upper_bound <span style="color:#f92672">-</span> S0_lower_bound) <span style="color:#f92672">*</span> uniform_samples <span style="color:#f92672">+</span> S0_lower_bound
delta_values <span style="color:#f92672">=</span> delta_call_BS(S0_values, K, T, r, sigma)            
</code></pre></div><p>I setup up easy-to-use iterables using <code>torch.utils.data</code>.
As a loss function, I use the standard mean-squared error and for an optimizer I use stochastic gradient descent.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Create Data Loaders</span>
training_set <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>TensorDataset(torch<span style="color:#f92672">.</span>Tensor(uniform_samples), 
        torch<span style="color:#f92672">.</span>Tensor(delta_values))
training_loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(training_set, batch_size<span style="color:#f92672">=</span>batch_size,
                                              shuffle<span style="color:#f92672">=</span>True)

<span style="color:#75715e"># Define Loss Function and Optimizer</span>
criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MSELoss()
optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(net<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
</code></pre></div><p>With all of this in place, we can finally train the network to approximate the analytical delta.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(num_epochs):
    running_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
    <span style="color:#66d9ef">for</span> i, data <span style="color:#f92672">in</span> enumerate(training_loader, <span style="color:#ae81ff">0</span>):
        inputs, targets <span style="color:#f92672">=</span> data;
        <span style="color:#75715e"># Zero the parameter gradients</span>
        optimizer<span style="color:#f92672">.</span>zero_grad()
        
        <span style="color:#75715e"># Forward + Backward + Optimize</span>
        outputs <span style="color:#f92672">=</span> net(inputs)
        loss <span style="color:#f92672">=</span> criterion(outputs, targets)
        loss<span style="color:#f92672">.</span>backward()
        optimizer<span style="color:#f92672">.</span>step()
       
        <span style="color:#75715e"># Print statistics</span>
        running_loss <span style="color:#f92672">+</span><span style="color:#f92672">=</span> loss<span style="color:#f92672">.</span>item()
    
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74"></span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">[</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">] loss: </span><span style="color:#e6db74">%.6f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> (epoch <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, running_loss))
</code></pre></div><p>As can be seen below, our simple network manages to approximate the analytical Black-Scholes delta quite quickly.</p>
<p><img src="/img/bs_delta_1.gif#center" alt="Learning the Black-Scholes Delta"></p>
<h2 id="learning-a-one-step-hedge">Learning A One-step Hedge</h2>
<p>The next step is to attempt to learn the best hedge position without any knowledge of the analytical delta, but rather by trying to minimize the profit and loss.
The simplest case is a one-period model.</p>
<p>At $t_0$ we sell a call option for $C_0$ and buy $\delta_0$ units of the underlying stock, $S_0$. 
Then at $T_1$ we have to pay out the payoff of the option (if positive) and close out our position. Thus, the function we want to minimize looks like</p>
<p>$$
\delta_0(S_1 - S_0) + C_0 - (S_1 - K)^+
$$</p>
<p>This will be close to the Black-Scholes delta, with a difference accounting for the discrete-time nature of the hedge.
We need to price the call options at $t_0$ as well as simulate realizations for the underlying asset at $t_1$. 
I also construct new data loaders.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">uniform_samples <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(num_samples, <span style="color:#ae81ff">1</span>)
normal_samples <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(num_samples, <span style="color:#ae81ff">1</span>)
S0_values <span style="color:#f92672">=</span> (S0_upper_bound <span style="color:#f92672">-</span> S0_lower_bound) <span style="color:#f92672">*</span> uniform_samples <span style="color:#f92672">+</span> S0_lower_bound
S1_values <span style="color:#f92672">=</span> S0_values <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>exp((r <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> sigma <span style="color:#f92672">*</span><span style="color:#f92672">*</span><span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> T 
                               <span style="color:#f92672">+</span> sigma <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(T) <span style="color:#f92672">*</span> normal_samples)
call_values <span style="color:#f92672">=</span> price_call_BS(S0_values, K, T, r, sigma)

training_set <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>TensorDataset(torch<span style="color:#f92672">.</span>Tensor(uniform_samples),
        torch<span style="color:#f92672">.</span>Tensor(S0_values),                                      
        torch<span style="color:#f92672">.</span>Tensor(S1_values),
        torch<span style="color:#f92672">.</span>Tensor(call_values))
training_loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(training_set, batch_size<span style="color:#f92672">=</span>batch_size,
                                              shuffle<span style="color:#f92672">=</span>True)
</code></pre></div><p>There's no need to define a custom loss function, as you can cast the problem in terms of MSE.
Then we can train the network.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(num_epochs):
    running_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
    <span style="color:#66d9ef">for</span> i, data <span style="color:#f92672">in</span> enumerate(training_loader, <span style="color:#ae81ff">0</span>):
        inputs, S0, S1, C0 <span style="color:#f92672">=</span> data;
        <span style="color:#75715e"># Zero the parameter gradients</span>
        optimizer<span style="color:#f92672">.</span>zero_grad()
        
        <span style="color:#75715e"># Forward + Backward + Optimize</span>
        outputs <span style="color:#f92672">=</span> net(inputs)
        loss <span style="color:#f92672">=</span> criterion(outputs <span style="color:#f92672">*</span> (S1 <span style="color:#f92672">-</span> S0) <span style="color:#f92672">+</span> C0, 
                         torch<span style="color:#f92672">.</span>max(S1 <span style="color:#f92672">-</span> K, torch<span style="color:#f92672">.</span>zeros(batch_size, <span style="color:#ae81ff">1</span>)))
        loss<span style="color:#f92672">.</span>backward()
        optimizer<span style="color:#f92672">.</span>step()
       
        <span style="color:#75715e"># Print statistics</span>
        running_loss <span style="color:#f92672">+</span><span style="color:#f92672">=</span> loss<span style="color:#f92672">.</span>item()
    
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74"></span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">[</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">] loss: </span><span style="color:#e6db74">%.6f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> (epoch <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, running_loss))
</code></pre></div><p>The optimization here is not as smooth as for the previous case, which is to be expected considering
the additional randomness (the simulations of $S_1$) and non-linearity (we're further removed from the target function).
The convergence is illustrated below.</p>
<p><img src="/img/bs_hedge_1.gif#center" alt="Learning a One-step Hedge"></p>
<h2 id="next-steps">Next Steps</h2>
<p>Roughly:</p>
<ol>
<li>Implement a two-period hedge. This requires constructing a semi-recurrent network, for which I'll need additional PyTorch API knowledge.</li>
<li>Change the underlying model to Heston. For this, we'll need two-dimensional inputs, as we'll require a derivative trading instrument to hedge the volatility.</li>
</ol>
<h2 id="resources">Resources</h2>
<p>The simple neural network was visualized using</p>
<ul>
<li><a href="https://lutzroeder.github.io/netron/">Netron</a> and</li>
<li><a href="http://alexlenail.me/NN-SVG/index.html">NN SVG</a></li>
</ul>
<p>Both have browser-based implementations available. 
I edit the resulting SVGs using <a href="https://inkspace.org">Inkscape</a>.</p>
</article>

      <div class="book-footer justify-between">
  

  

  
</div>

<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  displayMath: [['$$','$$']],
  processEscapes: true,
  processEnvironments: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
  TeX: { equationNumbers: { autoNumber: "AMS" },
       extensions: ["AMSmath.js", "AMSsymbols.js"] }
}
});
MathJax.Hub.Queue(function() {
  
  
  
  var all = MathJax.Hub.getAllJax(), i;
  for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
  }
});

MathJax.Hub.Config({

TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script> 

      
    </div>

    
  

  <aside class="book-toc levels-3 fixed">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#a-first-network">A First Network</a></li>
    <li><a href="#learning-the-black-scholes-delta">Learning The Black-Scholes Delta</a></li>
    <li><a href="#learning-a-one-step-hedge">Learning A One-step Hedge</a></li>
    <li><a href="#next-steps">Next Steps</a></li>
    <li><a href="#resources">Resources</a></li>
  </ul>
</nav>
  </aside>



  </main>

  
</body>

</html>
